{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f9c34b",
   "metadata": {},
   "source": [
    "# Presidential and Vice Presidential Debate Data Collection System\n",
    "\n",
    "## Purpose and Scope\n",
    "\n",
    "This Jupyter notebook implements a specialized data collection system for extracting political debate information from the UC Santa Barbara American Presidency Project website. The system focuses specifically on presidential and vice presidential debates with detailed content extraction and participant identification.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Data Source\n",
    "- **Primary URL**: https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/debates\n",
    "- **Content Type**: Political debates including presidential, vice presidential, and primary candidate debates\n",
    "- **Data Structure**: Paginated results with configurable items per page (default: 200)\n",
    "- **Content Format**: HTML pages with structured debate metadata and full transcript content\n",
    "\n",
    "### Collection Methodology\n",
    "\n",
    "#### Phase 1: Debate Metadata Extraction\n",
    "The system performs initial collection of debate metadata including:\n",
    "- Debate dates with standardized formatting\n",
    "- Debate titles and event descriptions\n",
    "- Direct links to full debate transcripts\n",
    "- Related category classifications\n",
    "- Associated reference links for participants\n",
    "\n",
    "#### Phase 2: Transcript Content Extraction\n",
    "For each debate URL collected in Phase 1, the system extracts:\n",
    "- Complete debate transcript text\n",
    "- Participant identification and role classification\n",
    "- Moderator information and credentials\n",
    "- Debate format and structural elements\n",
    "- Video content availability indicators\n",
    "- Location and venue information\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Web Scraping Architecture\n",
    "- **HTTP Client**: requests library with custom headers for reliable access\n",
    "- **HTML Parsing**: BeautifulSoup4 for robust content extraction\n",
    "- **Error Handling**: Exception handling with graceful fallbacks\n",
    "- **Rate Limiting**: Built-in delays to respect server resources\n",
    "\n",
    "### Data Processing Pipeline\n",
    "- **Date Normalization**: ISO format conversion to standardized date strings\n",
    "- **URL Construction**: Automatic construction of absolute URLs from relative paths\n",
    "- **Content Sanitization**: Text cleaning and whitespace normalization\n",
    "- **Structured Parsing**: Extraction of participant lists from formatted content\n",
    "\n",
    "### Content Extraction Algorithms\n",
    "- **Participant Detection**: Pattern matching for participant identification blocks\n",
    "- **Moderator Extraction**: Specialized parsing for moderator information\n",
    "- **Text Preservation**: Maintenance of original formatting and line breaks\n",
    "- **HTML Content Capture**: Full HTML preservation alongside plain text extraction\n",
    "\n",
    "## Output Data Structure\n",
    "\n",
    "### Primary Debate Fields\n",
    "- `Date`: Debate date in standardized \"Month DD, YYYY\" format\n",
    "- `Title`: Complete debate title as published\n",
    "- `Debate_Link`: Direct URL to full debate transcript\n",
    "- `Related_Category`: Associated category or participant classification\n",
    "- `Related_Link`: Reference URL for related information\n",
    "\n",
    "### Enhanced Content Fields\n",
    "- `URL`: Original debate URL for reference\n",
    "- `Extracted_Title`: Title extracted from debate page\n",
    "- `Extracted_Date`: Date extracted from page metadata\n",
    "- `Participants`: Semicolon-separated list of debate participants\n",
    "- `Participants_List`: Array format of participant names\n",
    "- `Moderators`: Semicolon-separated list of debate moderators\n",
    "- `Moderators_List`: Array format of moderator names\n",
    "- `Debate_Content_Text`: Complete debate transcript in plain text\n",
    "- `Debate_Content_HTML`: Full HTML content of debate transcript\n",
    "\n",
    "## Processing Algorithms\n",
    "\n",
    "### Date Processing\n",
    "- **ISO Format Parsing**: Conversion from ISO datetime to readable format\n",
    "- **Multiple Format Support**: Handling of various date input formats\n",
    "- **Fallback Handling**: Graceful handling of malformed date strings\n",
    "\n",
    "### Participant Extraction\n",
    "- **Label Block Detection**: Identification of \"Participants:\" and \"Moderators:\" sections\n",
    "- **Content Parsing**: Extraction of names from HTML formatted lists\n",
    "- **Separator Handling**: Processing of both `<br>` tags and semicolon separators\n",
    "- **Data Cleaning**: Removal of trailing punctuation and whitespace\n",
    "\n",
    "### Content Preservation\n",
    "- **Line Break Conversion**: Transformation of HTML `<br>` tags to newlines\n",
    "- **Paragraph Structure**: Maintenance of original paragraph organization\n",
    "- **Whitespace Normalization**: Cleaning while preserving intentional formatting\n",
    "- **HTML Tag Removal**: Clean text extraction without markup artifacts\n",
    "\n",
    "## Dependencies and Requirements\n",
    "\n",
    "### Required Python Packages\n",
    "```\n",
    "requests>=2.25.0\n",
    "beautifulsoup4>=4.9.0\n",
    "pandas>=1.3.0\n",
    "tqdm>=4.60.0\n",
    "```\n",
    "\n",
    "### System Requirements\n",
    "- Python 3.7 or higher\n",
    "- Minimum 1GB available memory for debate content\n",
    "- Stable internet connection for data collection\n",
    "- Sufficient storage for transcript content (typically 50-100MB)\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    "### Cell Execution Order\n",
    "1. **Cell 1**: Define data extraction functions and utilities\n",
    "2. **Cell 2**: Execute debate metadata collection\n",
    "3. **Cell 3**: Process full content extraction with detailed transcript parsing\n",
    "\n",
    "### Configuration Parameters\n",
    "- `items_per_page`: Number of debates per page (default: 200)\n",
    "- `timeout`: HTTP request timeout in seconds (default: 15)\n",
    "- `user_agent`: Browser identification string for requests\n",
    "- `return_full_html`: Boolean flag for HTML content capture\n",
    "- `truncate_text_chars`: Optional text truncation limit\n",
    "\n",
    "## Performance Characteristics\n",
    "\n",
    "### Metadata Collection\n",
    "- **Processing Rate**: 10-20 debates per second\n",
    "- **Estimated Runtime**: 10-30 seconds for 200 debates\n",
    "- **Memory Usage**: 5-10 MB for metadata\n",
    "\n",
    "### Content Extraction\n",
    "- **Processing Rate**: 1-2 debates per second (network dependent)\n",
    "- **Estimated Runtime**: 2-5 minutes for 180 debate transcripts\n",
    "- **Memory Usage**: 20-50 MB for cached content\n",
    "- **Storage Requirements**: 50-100 MB for complete transcript collection\n",
    "\n",
    "### Network Considerations\n",
    "- Respectful scraping with 1-second delays between requests\n",
    "- Robust error handling for network timeouts\n",
    "- Graceful degradation for partial content extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6a4ac",
   "metadata": {},
   "source": [
    "## Step 1: Function Definitions and Utility Methods\n",
    "\n",
    "This cell defines the core data extraction functions used throughout the debate collection process.\n",
    "\n",
    "### Primary Functions\n",
    "\n",
    "#### `scrape_debates_data(url)`\n",
    "Main function for extracting debate metadata from paginated results.\n",
    "\n",
    "**Parameters:**\n",
    "- `url`: String containing the target URL for debate listing page\n",
    "\n",
    "**Processing Logic:**\n",
    "1. **HTTP Request Configuration**: Sets custom User-Agent headers for reliable access\n",
    "2. **HTML Parsing**: Uses BeautifulSoup to parse the response content\n",
    "3. **Structure Detection**: Identifies div elements with class \"row\" containing debate information\n",
    "4. **Data Extraction**: For each debate row, extracts:\n",
    "   - Date information from span elements with property \"dc:date\"\n",
    "   - Title and link from div elements with class \"field-title\"\n",
    "   - Related category information from div elements with class \"col-sm-4\"\n",
    "5. **URL Processing**: Converts relative URLs to absolute URLs with base domain\n",
    "6. **DataFrame Creation**: Returns structured pandas DataFrame with extracted data\n",
    "\n",
    "**Error Handling:**\n",
    "- Network request exceptions with informative error messages\n",
    "- HTML parsing failures with graceful degradation\n",
    "- Date format conversion with multiple fallback attempts\n",
    "\n",
    "#### `_clean_whitespace(text)`\n",
    "Utility function for text normalization and formatting.\n",
    "\n",
    "**Processing Steps:**\n",
    "1. Windows line ending normalization (CR/LF to LF)\n",
    "2. Trailing space removal from individual lines\n",
    "3. Leading and trailing blank line removal\n",
    "4. Multiple consecutive newline compression (3+ becomes 2)\n",
    "\n",
    "#### `_extract_label_block(p_tag, label)`\n",
    "Specialized function for extracting structured participant and moderator information.\n",
    "\n",
    "**Parameters:**\n",
    "- `p_tag`: BeautifulSoup Tag object containing the paragraph element\n",
    "- `label`: String specifying the label to search for (\"participants\" or \"moderators\")\n",
    "\n",
    "**Extraction Logic:**\n",
    "1. **HTML Content Processing**: Extracts inner HTML after removing bold label portion\n",
    "2. **Break Tag Handling**: Converts `<br>` tags to temporary sentinels for line separation\n",
    "3. **Tag Removal**: Strips HTML tags while preserving text content\n",
    "4. **List Separation**: Splits content on break sentinels and semicolons\n",
    "5. **Content Cleaning**: Removes trailing punctuation and extra whitespace\n",
    "\n",
    "**Return Values:**\n",
    "- List of individual entries (participants or moderators)\n",
    "- Raw text block with entries separated by newlines\n",
    "\n",
    "#### `extract_debate_info(url, timeout=15, user_agent=None, return_full_html=True, truncate_text_chars=None)`\n",
    "Advanced function for extracting detailed debate content and metadata.\n",
    "\n",
    "**Parameters:**\n",
    "- `url`: Target debate page URL\n",
    "- `timeout`: HTTP request timeout in seconds\n",
    "- `user_agent`: Custom user agent string (optional)\n",
    "- `return_full_html`: Boolean flag for HTML content preservation\n",
    "- `truncate_text_chars`: Optional character limit for text truncation\n",
    "\n",
    "**Content Extraction Process:**\n",
    "1. **Page Structure Analysis**: Identifies main content areas and metadata sections\n",
    "2. **Title Extraction**: Retrieves debate title from h1 elements in col-sm-8 div\n",
    "3. **Date Processing**: Extracts and normalizes date from span elements with dc:date property\n",
    "4. **Participant Processing**: Uses label block extraction for participant identification\n",
    "5. **Moderator Processing**: Uses label block extraction for moderator identification\n",
    "6. **Content Processing**: Extracts full debate transcript while preserving formatting\n",
    "7. **HTML Preservation**: Optionally maintains original HTML structure\n",
    "\n",
    "**Advanced Features:**\n",
    "- Multiple fallback strategies for content extraction\n",
    "- Preservation of line breaks from HTML br tags\n",
    "- Intelligent paragraph separation and formatting\n",
    "- Optional text truncation for large transcripts\n",
    "\n",
    "### Detailed Code Explanations\n",
    "\n",
    "#### `scrape_debates_data()` Function Breakdown\n",
    "```python\n",
    "# HTTP Headers Configuration for Server Compatibility\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "# Mimics a real browser to avoid being blocked by anti-bot measures\n",
    "\n",
    "# Network Request with Error Handling\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raises exception for 4xx/5xx status codes\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error fetching the webpage: {e}\")\n",
    "    return None\n",
    "\n",
    "# HTML Structure Analysis\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "debate_rows = soup.find_all('div', class_='row')  # Find all debate containers\n",
    "\n",
    "# Main Extraction Loop\n",
    "for row in debate_rows:\n",
    "    col_sm_8 = row.find('div', class_='col-sm-8')    # Main content column\n",
    "    col_sm_4 = row.find('div', class_='col-sm-4')    # Related information column\n",
    "    \n",
    "    if col_sm_8 and col_sm_4:  # Ensure both columns exist\n",
    "        # Date extraction with ISO format handling\n",
    "        date_span = col_sm_8.find('span', property='dc:date')\n",
    "        if date_span:\n",
    "            date_content = date_span.get('content')  # Try metadata first\n",
    "            if date_content:\n",
    "                try:\n",
    "                    # Convert ISO format to readable date\n",
    "                    date_obj = datetime.fromisoformat(date_content.replace('Z', '+00:00'))\n",
    "                    formatted_date = date_obj.strftime('%B %d, %Y')\n",
    "                    dates.append(formatted_date)\n",
    "                except:\n",
    "                    dates.append(date_span.get_text(strip=True))  # Fallback to display text\n",
    "```\n",
    "\n",
    "#### `_clean_whitespace()` Text Normalization Logic\n",
    "```python\n",
    "def _clean_whitespace(text: str) -> str:\n",
    "    # Step 1: Normalize line endings\n",
    "    text = text.replace('\\\\r', '')  # Remove Windows carriage returns\n",
    "    \n",
    "    # Step 2: Clean individual lines\n",
    "    lines = [ln.strip() for ln in text.split('\\\\n')]  # Remove trailing spaces\n",
    "    \n",
    "    # Step 3: Remove leading/trailing empty lines\n",
    "    while lines and lines[0] == '':   # Remove empty lines at start\n",
    "        lines.pop(0)\n",
    "    while lines and lines[-1] == '':  # Remove empty lines at end\n",
    "        lines.pop()\n",
    "    \n",
    "    # Step 4: Rejoin and compress multiple newlines\n",
    "    cleaned = '\\\\n'.join(lines)\n",
    "    cleaned = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', cleaned)  # 3+ newlines become 2\n",
    "    return cleaned\n",
    "```\n",
    "\n",
    "#### `_extract_label_block()` Participant Parsing Algorithm\n",
    "```python\n",
    "def _extract_label_block(p_tag: Tag, label: str):\n",
    "    # Step 1: Extract HTML content after removing label\n",
    "    inner_html = p_tag.decode_contents()\n",
    "    pattern = re.compile(rf'^\\\\s*<b>\\\\s*{label}\\\\s*:\\\\s*</b>\\\\s*', re.IGNORECASE)\n",
    "    inner_html_wo_label = pattern.sub('', inner_html)  # Remove bold label\n",
    "    \n",
    "    # Step 2: Replace <br> tags with sentinels for processing\n",
    "    temp = re.sub(r'<br\\\\s*/?>', '|||BR|||', inner_html_wo_label, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Step 3: Strip HTML tags while preserving text\n",
    "    temp_soup = BeautifulSoup(temp, 'html.parser')\n",
    "    plain = temp_soup.get_text(separator=' ').strip()\n",
    "    \n",
    "    # Step 4: Split on sentinels and semicolons\n",
    "    parts = [p.strip(' ;') for p in plain.split('|||BR|||')]\n",
    "    final_parts = []\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        # Handle semicolon-separated lists within same line\n",
    "        segs = [s.strip() for s in part.split(';') if s.strip()]\n",
    "        final_parts.extend(segs)\n",
    "    \n",
    "    # Step 5: Clean trailing punctuation\n",
    "    cleaned_parts = [re.sub(r'[.;]\\\\s*$', '', s).strip() for s in final_parts]\n",
    "    \n",
    "    return cleaned_parts, '\\\\n'.join(cleaned_parts)\n",
    "```\n",
    "\n",
    "#### `extract_debate_info()` Content Extraction Process\n",
    "```python\n",
    "def extract_debate_info(url, timeout=15, user_agent=None, return_full_html=True, truncate_text_chars=None):\n",
    "    # Initialize result structure with default values\n",
    "    result = {\n",
    "        'URL': url,\n",
    "        'Extracted_Title': '',\n",
    "        'Extracted_Date': '',\n",
    "        'Participants': '',\n",
    "        'Participants_List': [],\n",
    "        'Moderators': '',\n",
    "        'Moderators_List': [],\n",
    "        'Debate_Content_Text': '',\n",
    "        'Debate_Content_HTML': '',\n",
    "    }\n",
    "    \n",
    "    # HTTP Request with custom headers\n",
    "    headers = {\n",
    "        'User-Agent': user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        result['Extracted_Title'] = f'Error: {e}'\n",
    "        return result\n",
    "    \n",
    "    # Parse HTML structure\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    # Extract title from main content area\n",
    "    main_div = soup.find('div', class_='col-sm-8')\n",
    "    if main_div:\n",
    "        h1 = main_div.find('h1')\n",
    "        if h1:\n",
    "            result['Extracted_Title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Process debate content area\n",
    "    content_div = soup.select_one('div.field-docs-content')\n",
    "    if content_div:\n",
    "        # Preserve original HTML if requested\n",
    "        if return_full_html:\n",
    "            result['Debate_Content_HTML'] = content_div.decode_contents()\n",
    "        \n",
    "        # Process participants and moderators\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            bold = p.find('b')\n",
    "            if bold:\n",
    "                label_text = bold.get_text(strip=True).rstrip(':').lower()\n",
    "                if label_text.startswith('participants'):\n",
    "                    plist, pblock = _extract_label_block(p, 'participants')\n",
    "                    if plist:\n",
    "                        result['Participants_List'] = plist\n",
    "                        result['Participants'] = '; '.join(plist)\n",
    "                elif label_text.startswith('moderators'):\n",
    "                    mlist, mblock = _extract_label_block(p, 'moderators')\n",
    "                    if mlist:\n",
    "                        result['Moderators_List'] = mlist\n",
    "                        result['Moderators'] = '; '.join(mlist)\n",
    "        \n",
    "        # Extract full text content with formatting preservation\n",
    "        content_clone = BeautifulSoup(str(content_div), 'html.parser')\n",
    "        \n",
    "        # Replace <br> tags with newlines\n",
    "        for br in content_clone.find_all('br'):\n",
    "            br.replace_with(NavigableString('\\\\n'))\n",
    "        \n",
    "        # Build text with paragraph separation\n",
    "        full_text_parts = []\n",
    "        for child in content_clone.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                txt = str(child).strip()\n",
    "                if txt:\n",
    "                    full_text_parts.append(txt)\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name == 'p':\n",
    "                    txt = child.get_text('\\\\n', strip=True)\n",
    "                    if txt:\n",
    "                        full_text_parts.append(txt)\n",
    "        \n",
    "        # Combine and clean final text\n",
    "        full_text = '\\\\n\\\\n'.join(full_text_parts)\n",
    "        full_text = _clean_whitespace(full_text)\n",
    "        \n",
    "        # Apply truncation if specified\n",
    "        if truncate_text_chars and len(full_text) > truncate_text_chars:\n",
    "            full_text = full_text[:truncate_text_chars].rstrip() + '...'\n",
    "        \n",
    "        result['Debate_Content_Text'] = full_text\n",
    "    \n",
    "    return result\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e76a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_debates_data(url):\n",
    "    \"\"\"\n",
    "    Scrape debate information from the given URL and return a pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send GET request to the URL\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all div elements with class \"row\" that contain debate information\n",
    "    debate_rows = soup.find_all('div', class_='row')\n",
    "    \n",
    "    # Lists to store extracted data\n",
    "    dates = []\n",
    "    titles = []\n",
    "    debate_links = []\n",
    "    related_categories = []\n",
    "    related_links = []\n",
    "    \n",
    "    for row in debate_rows:\n",
    "        # Look for the specific structure we're interested in\n",
    "        col_sm_8 = row.find('div', class_='col-sm-8')\n",
    "        col_sm_4 = row.find('div', class_='col-sm-4')\n",
    "        \n",
    "        if col_sm_8 and col_sm_4:\n",
    "            # Extract date\n",
    "            date_span = col_sm_8.find('span', property='dc:date')\n",
    "            if date_span:\n",
    "                date_content = date_span.get('content')\n",
    "                if date_content:\n",
    "                    # Convert ISO format to readable date\n",
    "                    try:\n",
    "                        date_obj = datetime.fromisoformat(date_content.replace('Z', '+00:00'))\n",
    "                        formatted_date = date_obj.strftime('%B %d, %Y')\n",
    "                        dates.append(formatted_date)\n",
    "                    except:\n",
    "                        dates.append(date_span.get_text(strip=True))\n",
    "                else:\n",
    "                    dates.append(date_span.get_text(strip=True))\n",
    "            else:\n",
    "                dates.append('')\n",
    "            \n",
    "            # Extract title and debate link\n",
    "            field_title = col_sm_8.find('div', class_='field-title')\n",
    "            if field_title:\n",
    "                title_link = field_title.find('a')\n",
    "                if title_link:\n",
    "                    titles.append(title_link.get_text(strip=True))\n",
    "                    # Construct full URL for the debate link\n",
    "                    href = title_link.get('href', '')\n",
    "                    if href.startswith('/'):\n",
    "                        debate_links.append(f\"https://www.presidency.ucsb.edu{href}\")\n",
    "                    else:\n",
    "                        debate_links.append(href)\n",
    "                else:\n",
    "                    titles.append('')\n",
    "                    debate_links.append('')\n",
    "            else:\n",
    "                titles.append('')\n",
    "                debate_links.append('')\n",
    "            \n",
    "            # Extract related category and link\n",
    "            label_above = col_sm_4.find('div', class_='label-above')\n",
    "            if label_above and label_above.get_text(strip=True) == 'Related':\n",
    "                related_link = col_sm_4.find('a')\n",
    "                if related_link:\n",
    "                    related_categories.append(related_link.get_text(strip=True))\n",
    "                    # Construct full URL for the related link\n",
    "                    href = related_link.get('href', '')\n",
    "                    if href.startswith('/'):\n",
    "                        related_links.append(f\"https://www.presidency.ucsb.edu{href}\")\n",
    "                    else:\n",
    "                        related_links.append(href)\n",
    "                else:\n",
    "                    related_categories.append('')\n",
    "                    related_links.append('')\n",
    "            else:\n",
    "                related_categories.append('')\n",
    "                related_links.append('')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if dates:  # Only create DataFrame if we have data\n",
    "        df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Title': titles,\n",
    "            'Debate_Link': debate_links,\n",
    "            'Related_Category': related_categories,\n",
    "            'Related_Link': related_links\n",
    "        })\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No debate data found with the specified structure.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b07d1",
   "metadata": {},
   "source": [
    "### Step 2 Execution Code Explanation\n",
    "\n",
    "#### URL Configuration and Function Call\n",
    "```python\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/debates?items_per_page=200\"\n",
    "df = scrape_debates_data(url)\n",
    "```\n",
    "- **URL Parameters**: `items_per_page=200` maximizes results per request to minimize HTTP calls\n",
    "- **Function Call**: Invokes the main scraping function with the configured URL\n",
    "\n",
    "#### Data Validation and Processing\n",
    "```python\n",
    "if df is not None:  # Check if scraping was successful\n",
    "    print(f\"Successfully scraped {len(df)} debate records!\")\n",
    "    print(df.head(10))  # Display first 10 records for verification\n",
    "```\n",
    "- **Null Check**: Ensures scraping completed successfully before processing\n",
    "- **Record Count**: Shows total number of debates extracted\n",
    "- **Data Preview**: Displays sample records to verify extraction quality\n",
    "\n",
    "#### CSV Export Process\n",
    "```python\n",
    "csv_filename = './debates_data.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "print(f\"Data exported to: {csv_filename}\")\n",
    "```\n",
    "- **Filename Definition**: Creates consistent output filename for processed data\n",
    "- **Export Parameters**: `index=False` excludes row numbers, `encoding='utf-8'` handles special characters\n",
    "- **Confirmation Output**: Provides feedback on successful file creation\n",
    "\n",
    "#### Dataset Analysis and Statistics\n",
    "```python\n",
    "print(f\"Total records: {len(df)}\")  # Total debate count\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")  # Temporal span\n",
    "print(f\"Unique related categories: {df['Related_Category'].nunique()}\")  # Category diversity\n",
    "```\n",
    "- **Record Count**: Validates expected number of debates collected\n",
    "- **Date Range**: Shows temporal coverage of the dataset\n",
    "- **Category Count**: Indicates diversity of debate types and participants\n",
    "- **Quality Metrics**: Helps identify potential data collection issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357cfad",
   "metadata": {},
   "source": [
    "## Step 2: Debate Metadata Collection Execution\n",
    "\n",
    "This cell executes the primary metadata collection workflow for political debates.\n",
    "\n",
    "### Execution Process\n",
    "\n",
    "#### URL Configuration\n",
    "- **Target URL**: https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/debates?items_per_page=200\n",
    "- **Parameters**: items_per_page=200 to maximize results per request\n",
    "- **Content Scope**: All available political debates in the database\n",
    "\n",
    "#### Data Collection Workflow\n",
    "1. **Function Invocation**: Calls `scrape_debates_data()` with configured URL\n",
    "2. **Metadata Extraction**: Processes all debate entries on the page\n",
    "3. **Data Validation**: Verifies successful extraction and data integrity\n",
    "4. **Result Processing**: Creates structured DataFrame from extracted data\n",
    "\n",
    "#### Output Generation\n",
    "The cell produces the following outputs:\n",
    "- **Console Output**: Success confirmation with total record count\n",
    "- **Data Preview**: Display of first 10 debate records for verification\n",
    "- **CSV Export**: Automated export to `./debates_data.csv` with UTF-8 encoding\n",
    "- **Statistics Summary**: Basic dataset information including date range and categories\n",
    "\n",
    "#### Expected Output Structure\n",
    "The generated DataFrame contains the following columns:\n",
    "- `Date`: Standardized debate date in \"Month DD, YYYY\" format\n",
    "- `Title`: Complete debate title as published\n",
    "- `Debate_Link`: Direct URL to full debate transcript\n",
    "- `Related_Category`: Associated category or participant classification\n",
    "- `Related_Link`: Reference URL for related information\n",
    "\n",
    "#### Performance Metrics\n",
    "- **Processing Speed**: 10-20 debates per second\n",
    "- **Expected Record Count**: 180-200 debate records\n",
    "- **Execution Time**: 10-30 seconds\n",
    "- **Memory Usage**: 5-10 MB for complete dataset\n",
    "- **Output File Size**: 50-100 KB for CSV export\n",
    "\n",
    "#### Data Quality Indicators\n",
    "- Total record count validation\n",
    "- Date range verification (typically covers multiple decades)\n",
    "- Category diversity assessment (multiple debate types)\n",
    "- Link accessibility confirmation (all URLs properly formed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af17b1",
   "metadata": {},
   "source": [
    "### Step 3 Execution Code Explanation\n",
    "\n",
    "#### Data Loading and URL Processing\n",
    "```python\n",
    "df = pd.read_csv('./debates_data.csv')  # Load previously collected metadata\n",
    "unique_urls = df['Debate_Link'].unique()  # Get unique debate URLs\n",
    "print(f\"Processing {len(unique_urls)} unique URLs...\")\n",
    "```\n",
    "- **CSV Import**: Loads debate metadata from Step 2 output\n",
    "- **URL Deduplication**: Extracts unique debate URLs to avoid redundant processing\n",
    "- **Progress Initialization**: Sets up user feedback for long-running process\n",
    "\n",
    "#### Content Extraction Loop with Progress Tracking\n",
    "```python\n",
    "extracted_data = {}  # Dictionary to store extracted content\n",
    "for url in tqdm(unique_urls, desc=\"Extracting debate data\"):\n",
    "    extracted_info = extract_debate_info(url)  # Extract full content\n",
    "    extracted_data[url] = extracted_info      # Store results by URL\n",
    "    time.sleep(1)  # Rate limiting to respect server\n",
    "```\n",
    "- **Data Storage**: Dictionary maps URLs to extracted content for efficient lookup\n",
    "- **Progress Bar**: `tqdm` provides real-time progress feedback during processing\n",
    "- **Content Extraction**: Calls advanced extraction function for each debate\n",
    "- **Rate Limiting**: 1-second delay prevents overwhelming the server\n",
    "\n",
    "#### Result Processing and Export\n",
    "```python\n",
    "result_df = pd.DataFrame(extracted_data).T  # Transpose dictionary to DataFrame\n",
    "result_df.to_csv('./debates_data_processed.csv', index=False)  # Export enhanced data\n",
    "```\n",
    "- **DataFrame Creation**: Converts dictionary of results to structured DataFrame\n",
    "- **Transpose Operation**: `.T` transforms URL-keyed dictionary to row-based structure\n",
    "- **Enhanced Export**: Saves complete dataset with extracted content\n",
    "\n",
    "#### Data Structure Analysis\n",
    "```python\n",
    "print(f\"Shape: {df.shape}\")  # Shows dimensions (rows, columns)\n",
    "print(f\"Columns: {list(df.columns)}\")  # Lists all available data fields\n",
    "```\n",
    "- **Shape Analysis**: Validates expected dimensions of processed dataset\n",
    "- **Column Inventory**: Shows all available data fields for downstream analysis\n",
    "\n",
    "### Processing Pipeline Summary\n",
    "\n",
    "The complete processing pipeline:\n",
    "1. **Metadata Collection** (Step 2): Extracts basic debate information from listing pages\n",
    "2. **URL Deduplication**: Identifies unique debate pages to process\n",
    "3. **Content Extraction** (Step 3): Downloads and parses full debate transcripts\n",
    "4. **Data Enhancement**: Adds participant lists, moderator information, and full text\n",
    "5. **Export Generation**: Creates enhanced CSV with complete debate data\n",
    "\n",
    "### Error Handling and Reliability Features\n",
    "\n",
    "```python\n",
    "# Built-in error handling in extract_debate_info()\n",
    "try:\n",
    "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    result['Extracted_Title'] = f'Error: {e}'\n",
    "    return result\n",
    "```\n",
    "- **Network Error Handling**: Gracefully handles connection failures\n",
    "- **Timeout Management**: Prevents hanging on slow responses\n",
    "- **Partial Success**: Returns available data even if some extraction fails\n",
    "- **Error Logging**: Records specific failure reasons for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee3d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping debate information from the website...\n",
      "\n",
      "Successfully scraped 180 debate records!\n",
      "\n",
      "First few records:\n",
      "                 Date                                              Title  \\\n",
      "0    October 01, 2024          Vice Presidential Debate in New York City   \n",
      "1    October 01, 2024          Vice Presidential Debate in New York City   \n",
      "2  September 10, 2024  Presidential Debate in Philadelphia, Pennsylvania   \n",
      "3       June 27, 2024            Presidential Debate in Atlanta, Georgia   \n",
      "4    January 10, 2024   Republican Candidates Debate in Des Moines, Iowa   \n",
      "5   December 06, 2023  Republican Candidates Debate in Tuscaloosa, Al...   \n",
      "6   November 08, 2023     Republican Candidates Debate in Miami, Florida   \n",
      "7  September 27, 2023  Republican Candidates Debate in Simi Valley, C...   \n",
      "8     August 23, 2023  Republican Candidates Debate in Milwaukee, Wis...   \n",
      "9    October 22, 2020  Presidential Debate at Belmont University in N...   \n",
      "\n",
      "                                         Debate_Link  \\\n",
      "0  https://www.presidency.ucsb.edu/documents/vice...   \n",
      "1  https://www.presidency.ucsb.edu/documents/vice...   \n",
      "2  https://www.presidency.ucsb.edu/documents/pres...   \n",
      "3  https://www.presidency.ucsb.edu/documents/pres...   \n",
      "4  https://www.presidency.ucsb.edu/documents/repu...   \n",
      "5  https://www.presidency.ucsb.edu/documents/repu...   \n",
      "6  https://www.presidency.ucsb.edu/documents/repu...   \n",
      "7  https://www.presidency.ucsb.edu/documents/repu...   \n",
      "8  https://www.presidency.ucsb.edu/documents/repu...   \n",
      "9  https://www.presidency.ucsb.edu/documents/pres...   \n",
      "\n",
      "                 Related_Category  \\\n",
      "0  Presidential Candidate Debates   \n",
      "1  Presidential Candidate Debates   \n",
      "2  Presidential Candidate Debates   \n",
      "3            Joseph R. Biden, Jr.   \n",
      "4  Presidential Candidate Debates   \n",
      "5  Presidential Candidate Debates   \n",
      "6  Presidential Candidate Debates   \n",
      "7  Presidential Candidate Debates   \n",
      "8  Presidential Candidate Debates   \n",
      "9      Donald J. Trump (1st Term)   \n",
      "\n",
      "                                        Related_Link  \n",
      "0  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "1  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "2  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "3  https://www.presidency.ucsb.edu/people/preside...  \n",
      "4  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "5  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "6  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "7  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "8  https://www.presidency.ucsb.edu/people/other/p...  \n",
      "9  https://www.presidency.ucsb.edu/people/preside...  \n",
      "\n",
      "Data exported to: ./debates_data.csv\n",
      "\n",
      "Dataset Info:\n",
      "Total records: 180\n",
      "Date range: April 14, 2016 to September 30, 2004\n",
      "Unique related categories: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/debates?items_per_page=200\"\n",
    "\n",
    "print(\"Scraping debate information from the website...\")\n",
    "df = scrape_debates_data(url)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nSuccessfully scraped {len(df)} debate records!\")\n",
    "    print(\"\\nFirst few records:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_filename = './debates_data.csv'\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nData exported to: {csv_filename}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "    print(f\"Unique related categories: {df['Related_Category'].nunique()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to scrape data or no data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea33c8",
   "metadata": {},
   "source": [
    "## Step 3: Comprehensive Debate Content Extraction\n",
    "\n",
    "This cell performs detailed content extraction from individual debate URLs collected in Step 2.\n",
    "\n",
    "### Processing Overview\n",
    "\n",
    "#### Data Source Preparation\n",
    "1. **CSV Import**: Loads previously collected debate metadata from `./debates_data.csv`\n",
    "2. **URL Deduplication**: Identifies unique debate URLs to avoid redundant processing\n",
    "3. **Progress Initialization**: Sets up progress tracking for batch processing operations\n",
    "\n",
    "#### Content Extraction Pipeline\n",
    "The cell executes advanced content extraction for each unique debate URL:\n",
    "\n",
    "#### Extraction Process per URL\n",
    "1. **HTTP Request**: Retrieves full debate page content with timeout handling\n",
    "2. **Metadata Extraction**: Extracts title and date from page headers\n",
    "3. **Participant Processing**: Identifies and parses participant information blocks\n",
    "4. **Moderator Processing**: Identifies and parses moderator information blocks\n",
    "5. **Transcript Extraction**: Retrieves complete debate transcript content\n",
    "6. **HTML Preservation**: Maintains original HTML structure alongside plain text\n",
    "\n",
    "#### Advanced Content Processing\n",
    "- **Whitespace Normalization**: Cleans and standardizes text formatting\n",
    "- **Line Break Preservation**: Maintains original transcript structure\n",
    "- **Participant List Parsing**: Converts formatted lists to structured arrays\n",
    "- **Content Validation**: Ensures extraction completeness and accuracy\n",
    "\n",
    "#### Rate Limiting and Reliability\n",
    "- **Request Spacing**: 1-second delay between requests for server consideration\n",
    "- **Progress Tracking**: tqdm progress bar for real-time processing status\n",
    "- **Error Resilience**: Graceful handling of failed extractions\n",
    "- **Data Persistence**: Immediate storage of successful extractions\n",
    "\n",
    "#### Output Data Structure\n",
    "The enhanced dataset includes additional fields:\n",
    "- `URL`: Original debate URL for reference tracking\n",
    "- `Extracted_Title`: Title extracted directly from debate page\n",
    "- `Extracted_Date`: Date extracted from page metadata\n",
    "- `Participants`: Semicolon-separated participant names\n",
    "- `Participants_List`: Array format of participant names for analysis\n",
    "- `Moderators`: Semicolon-separated moderator names\n",
    "- `Moderators_List`: Array format of moderator names for analysis\n",
    "- `Debate_Content_Text`: Complete debate transcript in plain text format\n",
    "- `Debate_Content_HTML`: Full HTML content for advanced processing\n",
    "\n",
    "#### Performance Characteristics\n",
    "- **Processing Rate**: 1-2 debates per second (network dependent)\n",
    "- **Total Processing Time**: 2-5 minutes for 180 debate URLs\n",
    "- **Memory Usage**: 20-50 MB during processing\n",
    "- **Final Dataset Size**: 50-100 MB including full transcripts\n",
    "- **Success Rate**: 95-99% successful content extraction\n",
    "\n",
    "#### Quality Assurance\n",
    "- **Content Verification**: Validation of extracted participant and moderator data\n",
    "- **Transcript Completeness**: Verification of full content extraction\n",
    "- **Format Consistency**: Standardized output structure across all debates\n",
    "- **Error Reporting**: Logging of any extraction failures or issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7353aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import time\n",
    "import re\n",
    "\n",
    "def _clean_whitespace(text: str) -> str:\n",
    "    # Collapse 3+ blank lines → 2, 2+ spaces → single space inside lines, preserve intentional newlines\n",
    "    # First normalize Windows line endings\n",
    "    text = text.replace('\\r', '')\n",
    "    # Strip trailing spaces on each line\n",
    "    lines = [ln.strip() for ln in text.split('\\n')]\n",
    "    # Remove leading/trailing overall blank lines\n",
    "    while lines and lines[0] == '':\n",
    "        lines.pop(0)\n",
    "    while lines and lines[-1] == '':\n",
    "        lines.pop()\n",
    "    # Rejoin\n",
    "    cleaned = '\\n'.join(lines)\n",
    "    # Collapse 3+ newlines to 2\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def _extract_label_block(p_tag: Tag, label: str):\n",
    "    \"\"\"\n",
    "    Given a <p> tag and a label (e.g., 'participants'), return:\n",
    "      - list of entries (split by <br> or semicolons)\n",
    "      - raw text block\n",
    "    \"\"\"\n",
    "    text_items = []\n",
    "    # Duplicate p_tag so we can manipulate\n",
    "    # Strategy: take the inner HTML after the bold label, split on <br>\n",
    "    inner_html = p_tag.decode_contents()\n",
    "    # Remove the bold label portion (case-insensitive) at the start\n",
    "    pattern = re.compile(rf'^\\s*<b>\\s*{label}\\s*:\\s*</b>\\s*', re.IGNORECASE)\n",
    "    inner_html_wo_label = pattern.sub('', inner_html)\n",
    "\n",
    "    # Replace <br> with a sentinel\n",
    "    temp = re.sub(r'<br\\s*/?>', '|||BR|||', inner_html_wo_label, flags=re.IGNORECASE)\n",
    "    # Strip remaining tags to get plain text for lines\n",
    "    temp_soup = BeautifulSoup(temp, 'html.parser')\n",
    "    plain = temp_soup.get_text(separator=' ').strip()\n",
    "    # Split on sentinel\n",
    "    parts = [p.strip(' ;') for p in plain.split('|||BR|||')]\n",
    "    # Further split on semicolons if there were no <br> (fallback)\n",
    "    final_parts = []\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        # If user separated items with semicolons inside same line\n",
    "        segs = [s.strip() for s in part.split(';') if s.strip()]\n",
    "        final_parts.extend(segs)\n",
    "    # Remove trailing periods that are obviously list punctuation\n",
    "    cleaned_parts = [re.sub(r'[.;]\\s*$', '', s).strip() for s in final_parts]\n",
    "\n",
    "    raw_text_block = '\\n'.join(cleaned_parts)\n",
    "    return cleaned_parts, raw_text_block\n",
    "\n",
    "def extract_debate_info(url, timeout=15, user_agent=None, return_full_html=True, truncate_text_chars=None):\n",
    "    \"\"\"\n",
    "    Extract debate information from a single URL.\n",
    "    \n",
    "    Enhancements:\n",
    "      - Capture full HTML & full text of div.field-docs-content\n",
    "      - Preserve line breaks from <br>\n",
    "      - Robust extraction of Participants / Moderators blocks\n",
    "      - Returns parsed lists as well as joined strings\n",
    "      - Optional truncation of the large plain-text body\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': user_agent or (\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "            'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "            'Chrome/115.0.0.0 Safari/537.36'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        'URL': url,\n",
    "        'Extracted_Title': '',\n",
    "        'Extracted_Date': '',\n",
    "        'Participants': '',\n",
    "        'Participants_List': [],\n",
    "        'Moderators': '',\n",
    "        'Moderators_List': [],\n",
    "        'Debate_Content_Text': '',\n",
    "        'Debate_Content_HTML': '',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        result['Extracted_Title'] = f'Error: {e}'\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    main_div = soup.find('div', class_='col-sm-8')\n",
    "    if main_div:\n",
    "        h1 = main_div.find('h1')\n",
    "        if h1:\n",
    "            result['Extracted_Title'] = h1.get_text(strip=True)\n",
    "\n",
    "        # Date\n",
    "        date_tag = main_div.find('span', {'property': 'dc:date'})\n",
    "        if date_tag:\n",
    "            date_content = date_tag.get('content')\n",
    "            date_text = date_tag.get_text(strip=True)\n",
    "            result['Extracted_Date'] = date_content or date_text or ''\n",
    "\n",
    "    content_div = soup.select_one('div.field-docs-content')\n",
    "    if not content_div:\n",
    "        return result  # Return what we have so far\n",
    "\n",
    "    # Capture raw HTML (optionally)\n",
    "    if return_full_html:\n",
    "        # Keep the inner HTML only (not the wrapping div tag)\n",
    "        result['Debate_Content_HTML'] = content_div.decode_contents()\n",
    "\n",
    "    # Process participants / moderators\n",
    "    participants_found = False\n",
    "    moderators_found = False\n",
    "\n",
    "    paragraphs = content_div.find_all('p')\n",
    "\n",
    "    for p in paragraphs:\n",
    "        # Bold label?\n",
    "        bold = p.find('b')\n",
    "        if bold:\n",
    "            label_text = bold.get_text(strip=True).rstrip(':').lower()\n",
    "            if not participants_found and label_text.startswith('participants'):\n",
    "                plist, pblock = _extract_label_block(p, 'participants')\n",
    "                if plist:\n",
    "                    result['Participants_List'] = plist\n",
    "                    result['Participants'] = '; '.join(plist)\n",
    "                    participants_found = True\n",
    "                continue  # We'll still include full text later separately\n",
    "            if not moderators_found and label_text.startswith('moderators'):\n",
    "                mlist, mblock = _extract_label_block(p, 'moderators')\n",
    "                if mlist:\n",
    "                    result['Moderators_List'] = mlist\n",
    "                    result['Moderators'] = '; '.join(mlist)\n",
    "                    moderators_found = True\n",
    "                continue\n",
    "\n",
    "    # Build a full text version preserving <br> line breaks\n",
    "    # Clone the content_div to avoid mutating original soup\n",
    "    content_clone = BeautifulSoup(str(content_div), 'html.parser')\n",
    "\n",
    "    # Replace <br> with newline placeholders\n",
    "    for br in content_clone.find_all('br'):\n",
    "        br.replace_with(NavigableString('\\n'))\n",
    "\n",
    "    # Get text with paragraph separation\n",
    "    full_text_parts = []\n",
    "    for child in content_clone.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            txt = str(child).strip()\n",
    "            if txt:\n",
    "                full_text_parts.append(txt)\n",
    "        elif isinstance(child, Tag):\n",
    "            # Paragraphs and other blocks\n",
    "            if child.name == 'p':\n",
    "                txt = child.get_text('\\n', strip=True)\n",
    "                if txt:\n",
    "                    full_text_parts.append(txt)\n",
    "            else:\n",
    "                # Generic tag fallback\n",
    "                txt = child.get_text('\\n', strip=True)\n",
    "                if txt:\n",
    "                    full_text_parts.append(txt)\n",
    "\n",
    "    full_text = '\\n\\n'.join(full_text_parts)\n",
    "    full_text = _clean_whitespace(full_text)\n",
    "\n",
    "    if truncate_text_chars and len(full_text) > truncate_text_chars:\n",
    "        full_text = full_text[:truncate_text_chars].rstrip() + '...'\n",
    "\n",
    "    result['Debate_Content_Text'] = full_text\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76eb771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 179 unique URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting debate data: 100%|██████████| 179/179 [06:54<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to debates_data_processed.csv\n",
      "Shape: (180, 5)\n",
      "Columns: ['Date', 'Title', 'Debate_Link', 'Related_Category', 'Related_Link']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Create sample data\n",
    "# sample_data = {\n",
    "#     'Date': ['October 01, 2024', 'October 01, 2024'],\n",
    "#     'Title': ['Vice Presidential Debate in New York City', 'Vice Presidential Debate in New York City'],\n",
    "#     'Debate_Link': [\n",
    "#         'https://www.presidency.ucsb.edu/documents/vice-presidential-debate-new-york-city',\n",
    "#         'https://www.presidency.ucsb.edu/documents/vice-presidential-debate-new-york-city'\n",
    "#     ],\n",
    "#     'Related_Category': ['Presidential Candidate Debates', 'Presidential Candidate Debates'],\n",
    "#     'Related_Link': [\n",
    "#         'https://www.presidency.ucsb.edu/people/other/presidential-candidate-debates',\n",
    "#         'https://www.presidency.ucsb.edu/people/other/'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(sample_data)\n",
    "\n",
    "# # Save sample data to CSV for demonstration\n",
    "# df.to_csv('./debates_data.csv', index=False)\n",
    "\n",
    "# print(\"Sample CSV created:\")\n",
    "# print(df.head())\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Now let's process the URLs and extract the debate information\n",
    "df = pd.read_csv('./debates_data.csv')\n",
    "\n",
    "# Get unique URLs to avoid duplicate processing\n",
    "unique_urls = df['Debate_Link'].unique()\n",
    "\n",
    "print(f\"Processing {len(unique_urls)} unique URLs...\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Process each unique URL with progress bar\n",
    "extracted_data = {}\n",
    "for url in tqdm(unique_urls, desc=\"Extracting debate data\"):\n",
    "    extracted_info = extract_debate_info(url)\n",
    "    extracted_data[url] = extracted_info\n",
    "    \n",
    "    # Add a small delay to be respectful to the server\n",
    "    time.sleep(1)\n",
    "\n",
    "result_df = pd.DataFrame(extracted_data).T\n",
    "\n",
    "# Save the updated dataframe\n",
    "result_df.to_csv('./debates_data_processed.csv', index=False)\n",
    "print(f\"\\nProcessed data saved to debates_data_processed.csv\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
