{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fa449b",
   "metadata": {},
   "source": [
    "# Campaign Documents Data Collection System\n",
    "\n",
    "## Purpose and Scope\n",
    "\n",
    "This Jupyter notebook implements a data collection system for extracting political campaign documents from the UC Santa Barbara American Presidency Project website (presidency.ucsb.edu). The system is designed to systematically collect campaign documents with associated metadata and full content text for analysis purposes.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Data Source\n",
    "- **Primary URL**: https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/campaign-documents\n",
    "- **Data Structure**: Paginated results with 1000 documents per page\n",
    "- **Content Format**: HTML pages with structured metadata and full document text\n",
    "\n",
    "### Collection Methodology\n",
    "\n",
    "#### Phase 1: Metadata Extraction\n",
    "The system performs initial collection of document metadata including:\n",
    "- Document publication dates\n",
    "- Document titles and classifications\n",
    "- Direct links to full document content\n",
    "- Related category assignments\n",
    "- Associated reference links\n",
    "\n",
    "#### Phase 2: Content Extraction\n",
    "For each document URL collected in Phase 1, the system extracts:\n",
    "- Complete document text content\n",
    "- Speaker identification and titles\n",
    "- Document type classification (Speech, Remarks, Statement, Interview, Address, Debate)\n",
    "- Location information extracted from titles\n",
    "- Video content availability detection\n",
    "- Word count calculations\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Threading Architecture\n",
    "- **Concurrent Processing**: ThreadPoolExecutor with configurable worker threads\n",
    "- **Connection Pooling**: Session-based HTTP requests with retry strategies\n",
    "- **Rate Limiting**: Built-in delays to respect server resources\n",
    "\n",
    "### Data Persistence\n",
    "- **Caching System**: Pickle-based storage to avoid re-processing existing data\n",
    "- **Checkpoint Management**: JSON-based progress tracking for resumable operations\n",
    "- **Output Format**: CSV files with UTF-8 encoding\n",
    "\n",
    "### Error Handling\n",
    "- **Network Resilience**: Automatic retries with exponential backoff\n",
    "- **Data Validation**: Content verification and error status tracking\n",
    "- **Graceful Degradation**: Partial success handling for incomplete extractions\n",
    "\n",
    "## Output Data Structure\n",
    "\n",
    "### Primary Metadata Fields\n",
    "- `Date`: Document publication date in standardized format\n",
    "- `Title`: Complete document title as published\n",
    "- `Document_Link`: Direct URL to full document content\n",
    "- `Related_Category`: Associated political figure or category\n",
    "- `Related_Link`: Reference URL for related category\n",
    "- `Page`: Source page number for tracking purposes\n",
    "- `URL_Page_Index`: Zero-based page index for URL construction\n",
    "\n",
    "### Enhanced Content Fields\n",
    "- `Document_Title`: Extracted title from document page\n",
    "- `Document_Date`: Parsed date from document metadata\n",
    "- `Document_Content`: Full extracted text content\n",
    "- `Speaker`: Identified speaker or author\n",
    "- `Speaker_Title`: Official title or position\n",
    "- `Document_Type`: Classified document category\n",
    "- `Location`: Extracted location information\n",
    "- `Video_Available`: Boolean flag for video content presence\n",
    "- `Word_Count`: Total word count of document content\n",
    "- `Extraction_Status`: Processing status indicator\n",
    "\n",
    "## Dependencies and Requirements\n",
    "\n",
    "### Required Python Packages\n",
    "```\n",
    "requests>=2.25.0\n",
    "beautifulsoup4>=4.9.0\n",
    "pandas>=1.3.0\n",
    "tqdm>=4.60.0\n",
    "```\n",
    "\n",
    "### System Requirements\n",
    "- Python 3.7 or higher\n",
    "- Minimum 2GB available memory for large datasets\n",
    "- Stable internet connection for data collection\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    "### Cell Execution Order\n",
    "1. **Cell 1**: Import dependencies and initialize CampaignDocumentsScraper class\n",
    "2. **Cell 2**: Execute metadata collection pipeline\n",
    "3. **Cell 3**: Process full content extraction (optional)\n",
    "\n",
    "### Configuration Parameters\n",
    "- `max_workers`: Number of concurrent threads (default: 5)\n",
    "- `max_pages`: Maximum pages to process (default: 25)\n",
    "- `batch_size`: URLs processed per batch (default: 50)\n",
    "- `save_interval`: Progress save frequency (default: 500)\n",
    "\n",
    "## Performance Characteristics\n",
    "\n",
    "### Metadata Collection\n",
    "- **Processing Rate**: Approximately 400-500 documents per second\n",
    "- **Estimated Runtime**: 1-2 minutes for 25,000 documents\n",
    "- **Memory Usage**: 10-15 MB for complete dataset\n",
    "\n",
    "### Content Extraction\n",
    "- **Processing Rate**: 2-3 documents per second (network dependent)\n",
    "- **Estimated Runtime**: 30-45 minutes for 7,500 unique documents\n",
    "- **Memory Usage**: 50-100 MB for cached content\n",
    "\n",
    "### Network Considerations\n",
    "- Respectful scraping with built-in delays\n",
    "- Automatic retry mechanisms for failed requests\n",
    "- Session persistence for connection efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0bb30",
   "metadata": {},
   "source": [
    "## Step 1: Library Imports and Class Definitions\n",
    "\n",
    "This cell initializes the required dependencies and defines the primary data collection class. The implementation includes the following components:\n",
    "\n",
    "### Import Dependencies\n",
    "- **requests**: HTTP client library for web requests\n",
    "- **BeautifulSoup**: HTML parsing and content extraction\n",
    "- **pandas**: Data structure manipulation and CSV export\n",
    "- **threading**: Thread-safe operations for concurrent processing\n",
    "- **concurrent.futures**: ThreadPoolExecutor for parallel request handling\n",
    "- **logging**: Structured logging for process monitoring\n",
    "\n",
    "### CampaignDocumentsScraper Class Architecture\n",
    "\n",
    "The main class implements the following methods and features:\n",
    "\n",
    "#### Core Methods\n",
    "- `__init__()`: Initialize session configuration and threading parameters\n",
    "- `get_total_pages()`: Determine available pages through pagination analysis\n",
    "- `test_page_availability()`: Binary search algorithm for page boundary detection\n",
    "- `scrape_page()`: Extract document metadata from individual pages\n",
    "- `scrape_all_pages()`: Orchestrate multi-threaded page processing\n",
    "\n",
    "#### Session Management\n",
    "- **HTTP Headers**: User-Agent rotation and standard browser headers\n",
    "- **Connection Pooling**: Persistent session for request efficiency\n",
    "- **Request Timeout**: Configurable timeout handling for network resilience\n",
    "\n",
    "#### Thread Safety Features\n",
    "- **Data Lock**: Mutex protection for shared data structures\n",
    "- **Worker Management**: Configurable thread pool sizing\n",
    "- **Rate Limiting**: Server-respectful request spacing\n",
    "\n",
    "### Detailed Code Explanation\n",
    "\n",
    "#### Import Section\n",
    "```python\n",
    "import requests                    # HTTP library for making web requests\n",
    "from bs4 import BeautifulSoup     # HTML parser for extracting data from web pages\n",
    "import pandas as pd               # Data manipulation and CSV export\n",
    "import time                       # Time utilities for delays and timing\n",
    "from datetime import datetime     # Date parsing and formatting\n",
    "import re                         # Regular expressions for pattern matching\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # Multi-threading\n",
    "import threading                  # Thread synchronization primitives\n",
    "from urllib.parse import urljoin, urlparse  # URL manipulation utilities\n",
    "import logging                    # Structured logging system\n",
    "```\n",
    "\n",
    "#### Class Constructor Explanation\n",
    "The `__init__` method sets up the scraper with:\n",
    "- **base_url**: The target website URL to scrape\n",
    "- **max_workers**: Number of concurrent threads (default: 5)\n",
    "- **max_pages**: Maximum pages to process (default: 25)\n",
    "- **session**: Persistent HTTP session with browser-like headers\n",
    "- **data_lock**: Thread synchronization lock for safe data access\n",
    "- **all_documents**: List to store all scraped document data\n",
    "\n",
    "#### HTTP Session Configuration\n",
    "```python\n",
    "self.session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0...',    # Mimics a real browser\n",
    "    'Accept': 'text/html...',          # Specifies accepted content types\n",
    "    'Accept-Language': 'en-US,en',     # Language preferences\n",
    "    'Accept-Encoding': 'gzip, deflate', # Compression support\n",
    "    'Connection': 'keep-alive',        # Persistent connections\n",
    "    'Upgrade-Insecure-Requests': '1'   # HTTPS preference\n",
    "})\n",
    "```\n",
    "\n",
    "#### Pagination Detection Logic\n",
    "The `get_total_pages()` method uses multiple strategies:\n",
    "1. **HTML Pagination Parser**: Searches for `<ul class=\"pagination\">` elements\n",
    "2. **Link Analysis**: Extracts page numbers from pagination links using regex\n",
    "3. **Document Count Validation**: Verifies page contains expected number of documents\n",
    "4. **Binary Search Fallback**: Tests page availability when pagination is unclear\n",
    "\n",
    "#### Page Scraping Algorithm\n",
    "The `scrape_page()` method processes individual pages:\n",
    "1. **URL Construction**: Builds page-specific URLs with zero-based indexing\n",
    "2. **HTML Retrieval**: Makes HTTP request with timeout and error handling\n",
    "3. **DOM Parsing**: Uses BeautifulSoup to parse HTML structure\n",
    "4. **Data Extraction**: Finds document rows and extracts metadata fields\n",
    "5. **Date Processing**: Parses various date formats and standardizes output\n",
    "6. **Link Processing**: Converts relative URLs to absolute URLs\n",
    "\n",
    "#### Multi-threading Coordination\n",
    "The `scrape_all_pages()` method orchestrates concurrent processing:\n",
    "1. **Worker Pool Creation**: Initializes ThreadPoolExecutor with optimal thread count\n",
    "2. **Task Submission**: Submits each page as a separate task\n",
    "3. **Result Aggregation**: Collects completed results in thread-safe manner\n",
    "4. **Progress Tracking**: Logs completion status and document counts\n",
    "5. **Error Handling**: Gracefully handles individual page failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67da10f",
   "metadata": {},
   "source": [
    "### Detailed Line-by-Line Code Explanation\n",
    "\n",
    "#### Key Method Explanations\n",
    "\n",
    "**`get_total_pages()` Method Logic:**\n",
    "```python\n",
    "# Method 1: Look for pagination HTML elements\n",
    "pagination = soup.find('ul', class_='pagination')\n",
    "# Searches for standard pagination navigation in the page footer\n",
    "\n",
    "# Extract last page number from pagination links\n",
    "page_match = re.search(r'page=(\\\\d+)', last_href)\n",
    "# Uses regex to find \"page=NUMBER\" pattern in URL parameters\n",
    "\n",
    "# Method 2: Count documents on first page as fallback\n",
    "document_count = 0\n",
    "for row in rows:\n",
    "    main_col = row.find('div', class_='col-sm-8')\n",
    "    if main_col and main_col.find('div', class_='field-title'):\n",
    "        document_count += 1\n",
    "# Counts actual document entries to validate page structure\n",
    "```\n",
    "\n",
    "**`test_page_availability()` Binary Search:**\n",
    "```python\n",
    "left, right = 1, self.max_pages  # Set search boundaries\n",
    "while left <= right:             # Binary search loop\n",
    "    mid = (left + right) // 2    # Calculate middle page\n",
    "    test_url = f\"{self.base_url}&page={mid - 1}\"  # Build test URL\n",
    "    # Tests if page exists by checking for document content\n",
    "```\n",
    "\n",
    "**`scrape_page()` Data Extraction Process:**\n",
    "```python\n",
    "# Find main content areas in HTML structure\n",
    "main_col = row.find('div', class_='col-sm-8')    # Document content column\n",
    "related_col = row.find('div', class_='col-sm-4')  # Related links column\n",
    "\n",
    "# Extract date with multiple fallback strategies\n",
    "date_element = main_col.find('span', {'property': 'dc:date'})\n",
    "date_content = date_element.get('content', '')     # Try metadata first\n",
    "date_text = date_element.get_text(strip=True)      # Fallback to display text\n",
    "\n",
    "# Process title and document link\n",
    "link_element = title_element.find('a')             # Find link within title\n",
    "title = link_element.get_text(strip=True)          # Extract clean title text\n",
    "document_link = urljoin('https://www.presidency.ucsb.edu', href)  # Make absolute URL\n",
    "```\n",
    "\n",
    "**`scrape_all_pages()` Threading Coordination:**\n",
    "```python\n",
    "# Create thread pool with optimal worker count\n",
    "actual_workers = min(self.max_workers, total_pages, 6)\n",
    "with ThreadPoolExecutor(max_workers=actual_workers) as executor:\n",
    "    \n",
    "    # Submit all page scraping tasks simultaneously\n",
    "    future_to_page = {\n",
    "        executor.submit(self.scrape_page, page_num): page_num \n",
    "        for page_num in range(1, total_pages + 1)\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete (not in order)\n",
    "    for future in as_completed(future_to_page):\n",
    "        page_documents = future.result()           # Get results from completed thread\n",
    "        with self.data_lock:                       # Thread-safe data access\n",
    "            self.all_documents.extend(page_documents)  # Add to main collection\n",
    "```\n",
    "\n",
    "**Date Parsing Logic:**\n",
    "```python\n",
    "def parse_date(self, date_string):\n",
    "    if 'T' in date_string:  # ISO format detection\n",
    "        return datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "    else:  # Standard date format\n",
    "        return datetime.strptime(date_string, '%Y-%m-%d')\n",
    "    # Standardizes various date formats to consistent output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70817e37",
   "metadata": {},
   "source": [
    "## Step 2: Metadata Collection Pipeline Execution\n",
    "\n",
    "This cell executes the primary data collection workflow with the following operational sequence:\n",
    "\n",
    "### Initialization Phase\n",
    "1. **Scraper Configuration**: Initialize CampaignDocumentsScraper with parameters:\n",
    "   - Worker threads: 5 concurrent processors\n",
    "   - Page limit: 25 maximum pages\n",
    "   - Request timeout: 15 seconds per page\n",
    "\n",
    "### Discovery Phase\n",
    "2. **Pagination Analysis**: Determine total available pages through:\n",
    "   - HTML pagination element parsing\n",
    "   - Binary search for page boundary detection\n",
    "   - Maximum page constraint enforcement\n",
    "\n",
    "### Collection Phase\n",
    "3. **Concurrent Page Processing**: Execute multi-threaded data extraction:\n",
    "   - ThreadPoolExecutor manages worker threads\n",
    "   - Each worker processes individual pages\n",
    "   - Thread-safe data aggregation\n",
    "\n",
    "### Data Processing Phase\n",
    "4. **Metadata Extraction**: For each document, extract:\n",
    "   - Publication date with format standardization\n",
    "   - Document title and classification\n",
    "   - Direct URL to full content\n",
    "   - Related category assignments\n",
    "   - Reference link associations\n",
    "\n",
    "### Output Generation Phase\n",
    "5. **Result Compilation**: Generate structured output including:\n",
    "   - CSV file export with UTF-8 encoding\n",
    "   - Processing statistics and performance metrics\n",
    "   - Data quality validation results\n",
    "\n",
    "### Expected Performance Metrics\n",
    "- **Processing Rate**: 400-500 documents per second\n",
    "- **Execution Time**: 60-120 seconds for 25,000 documents\n",
    "- **Memory Consumption**: 10-15 MB for complete dataset\n",
    "- **Success Rate**: 99% successful metadata extraction\n",
    "- **Output File**: `campaign_documents_[count]docs_[timestamp].csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969f90c",
   "metadata": {},
   "source": [
    "## Step 3: Content Extraction Pipeline Execution\n",
    "\n",
    "This cell executes the advanced content extraction workflow for individual campaign document URLs collected in the metadata phase.\n",
    "\n",
    "### Processing Overview\n",
    "\n",
    "#### Data Source Preparation\n",
    "1. **CSV Import**: Loads previously collected campaign document metadata from CSV files\n",
    "2. **URL Validation**: Filters valid document URLs for content extraction\n",
    "3. **Batch Configuration**: Organizes URLs into processing batches for efficient handling\n",
    "\n",
    "#### Content Extraction Architecture\n",
    "The system implements a multi-threaded content extraction pipeline:\n",
    "\n",
    "#### Extraction Process per Document\n",
    "1. **HTTP Request Execution**: Retrieves full document page content with timeout handling\n",
    "2. **Document Metadata Extraction**: Extracts title, date, and classification information\n",
    "3. **Speaker Information Processing**: Identifies document authors and their official titles\n",
    "4. **Content Text Extraction**: Retrieves complete document text with paragraph preservation\n",
    "5. **Document Type Classification**: Categorizes documents (Speech, Remarks, Statement, Interview, Address, Debate)\n",
    "6. **Location Information Extraction**: Parses location data from document titles and content\n",
    "7. **Video Content Detection**: Identifies presence of embedded video content\n",
    "8. **Word Count Calculation**: Computes total word count for analysis purposes\n",
    "\n",
    "#### Advanced Processing Features\n",
    "- **Caching System**: Pickle-based storage prevents re-processing of existing documents\n",
    "- **Checkpoint Management**: JSON-based progress tracking enables resumable operations\n",
    "- **Error Recovery**: Graceful handling of network failures and parsing errors\n",
    "- **Content Validation**: Verification of extraction completeness and data quality\n",
    "\n",
    "#### Threading and Performance Management\n",
    "- **Worker Thread Configuration**: Configurable number of concurrent processors (default: 8)\n",
    "- **Request Rate Limiting**: Built-in delays respect server resources and prevent blocking\n",
    "- **Progress Monitoring**: Real-time progress tracking with detailed logging\n",
    "- **Memory Management**: Efficient handling of large document collections\n",
    "\n",
    "#### Output Data Enhancement\n",
    "The enhanced dataset includes additional fields:\n",
    "- `Document_Title`: Title extracted directly from document page\n",
    "- `Document_Date`: Parsed date from document metadata\n",
    "- `Document_Content`: Complete document text content\n",
    "- `Speaker`: Identified document author or speaker\n",
    "- `Speaker_Title`: Official position or title of the speaker\n",
    "- `Document_Type`: Classified document category\n",
    "- `Location`: Extracted location information\n",
    "- `Video_Available`: Boolean indicator for video content presence\n",
    "- `Word_Count`: Total word count of document content\n",
    "- `Extraction_Status`: Processing status for quality assurance\n",
    "\n",
    "#### Performance Characteristics\n",
    "- **Processing Rate**: 2-3 documents per second (network dependent)\n",
    "- **Total Processing Time**: 30-45 minutes for 7,500 unique documents\n",
    "- **Memory Usage**: 50-100 MB during processing\n",
    "- **Cache Storage**: 20-50 MB for persistent caching\n",
    "- **Success Rate**: 95-99% successful content extraction\n",
    "\n",
    "#### Quality Assurance Measures\n",
    "- **Content Verification**: Validation of extracted text completeness\n",
    "- **Metadata Accuracy**: Cross-verification of extracted metadata\n",
    "- **Error Logging**: Detailed logging of processing failures\n",
    "- **Data Consistency**: Standardized output format across all documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566bb5a",
   "metadata": {},
   "source": [
    "### Step 2 Execution Code Explanation\n",
    "\n",
    "#### Scraper Initialization and Configuration\n",
    "```python\n",
    "base_url = \"https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/campaign-documents?items_per_page=1000\"\n",
    "scraper = CampaignDocumentsScraper(base_url, max_workers=5, max_pages=25)\n",
    "```\n",
    "- **base_url**: Target URL with 1000 items per page parameter for maximum efficiency\n",
    "- **max_workers=5**: Limits concurrent threads to avoid overwhelming the server\n",
    "- **max_pages=25**: Safety limit to prevent infinite loops or excessive requests\n",
    "\n",
    "#### Timing and Performance Measurement\n",
    "```python\n",
    "start_time = time.time()          # Record start time for performance measurement\n",
    "documents = scraper.scrape_all_pages()  # Execute main scraping operation\n",
    "end_time = time.time()            # Record completion time\n",
    "scraping_time = end_time - start_time    # Calculate total execution duration\n",
    "```\n",
    "\n",
    "#### Data Processing and Validation\n",
    "```python\n",
    "df = pd.DataFrame(documents)      # Convert raw data to structured DataFrame\n",
    "if not df.empty:                  # Validate that data was successfully collected\n",
    "    df = df.sort_values(['Page', 'Date'], ascending=[True, False])  # Sort for analysis\n",
    "```\n",
    "- **DataFrame Conversion**: Transforms list of dictionaries into structured data\n",
    "- **Empty Check**: Prevents errors if no data was collected\n",
    "- **Sorting**: Orders by page first, then by date (newest first within each page)\n",
    "\n",
    "#### Statistical Output Generation\n",
    "```python\n",
    "print(f\"Total documents scraped: {len(df):,}\")  # Total count with thousands separator\n",
    "print(f\"Pages scraped: {df['Page'].nunique()}\")  # Number of unique pages processed\n",
    "print(f\"Actual coverage: {len(df) / (25 * 1000) * 100:.1f}% of maximum possible\")\n",
    "```\n",
    "- **Document Count**: Shows total successfully scraped documents\n",
    "- **Page Count**: Verifies how many pages actually contained data\n",
    "- **Coverage Percentage**: Calculates efficiency against theoretical maximum\n",
    "\n",
    "#### Performance Metrics Calculation\n",
    "```python\n",
    "print(f\"Scraping speed: {len(df) / scraping_time:.1f} documents/second\")\n",
    "print(f\"Average documents per page: {len(df) / df['Page'].nunique():.0f}\")\n",
    "```\n",
    "- **Processing Speed**: Documents per second for performance assessment\n",
    "- **Page Efficiency**: Average documents per page to identify potential issues\n",
    "\n",
    "#### Data Analysis and Summarization\n",
    "```python\n",
    "# Page distribution analysis\n",
    "page_counts = df['Page'].value_counts().sort_index()\n",
    "for page, count in page_counts.items():\n",
    "    print(f\"Page {page}: {count:,} documents\")\n",
    "\n",
    "# Sample data display for verification\n",
    "sample_columns = ['Date', 'Title', 'Related_Category', 'Page']\n",
    "print(df[sample_columns].head().to_string(index=False, max_colwidth=60))\n",
    "```\n",
    "- **Page Distribution**: Shows document count per page to identify inconsistencies\n",
    "- **Sample Display**: Provides preview of extracted data for quality verification\n",
    "\n",
    "#### Data Export and File Management\n",
    "```python\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Create unique timestamp\n",
    "csv_filename = f'./campaign_documents_{len(df)}docs_{timestamp}.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')  # Export to CSV\n",
    "```\n",
    "- **Timestamp Creation**: Generates unique identifier for output files\n",
    "- **Filename Generation**: Includes document count and timestamp for clarity\n",
    "- **CSV Export**: Saves data with UTF-8 encoding to handle special characters\n",
    "\n",
    "#### Data Quality Analysis\n",
    "```python\n",
    "# Memory usage calculation\n",
    "memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "\n",
    "# Category distribution analysis\n",
    "category_counts = df['Related_Category'].value_counts().head(10)\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category}: {count:,} documents\")\n",
    "\n",
    "# Temporal distribution analysis\n",
    "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
    "year_counts = df['Year'].value_counts().sort_index().tail(10)\n",
    "```\n",
    "- **Memory Analysis**: Tracks resource usage for optimization\n",
    "- **Category Analysis**: Identifies most common document categories\n",
    "- **Temporal Analysis**: Shows document distribution across years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CampaignDocumentsScraper:\n",
    "  def __init__(self, base_url, max_workers=5, max_pages=25):\n",
    "      self.base_url = base_url\n",
    "      self.max_workers = max_workers\n",
    "      self.max_pages = max_pages  # Maximum expected pages\n",
    "      self.session = requests.Session()\n",
    "      self.session.headers.update({\n",
    "          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "          'Accept-Language': 'en-US,en;q=0.5',\n",
    "          'Accept-Encoding': 'gzip, deflate',\n",
    "          'Connection': 'keep-alive',\n",
    "          'Upgrade-Insecure-Requests': '1',\n",
    "      })\n",
    "      self.data_lock = threading.Lock()\n",
    "      self.all_documents = []\n",
    "  \n",
    "  def get_total_pages(self):\n",
    "      \"\"\"Get the total number of pages by checking pagination or testing pages\"\"\"\n",
    "      try:\n",
    "          response = self.session.get(self.base_url, timeout=10)\n",
    "          response.raise_for_status()\n",
    "          soup = BeautifulSoup(response.content, 'html.parser')\n",
    "          \n",
    "          # Method 1: Try to find pagination info\n",
    "          pagination = soup.find('ul', class_='pagination')\n",
    "          if pagination:\n",
    "              # Look for the last page link\n",
    "              last_link = pagination.find('li', class_='pager-last')\n",
    "              if last_link and last_link.find('a'):\n",
    "                  last_href = last_link.find('a')['href']\n",
    "                  page_match = re.search(r'page=(\\d+)', last_href)\n",
    "                  if page_match:\n",
    "                      # Pages are 0-indexed, so add 1 to get total pages\n",
    "                      total_pages = int(page_match.group(1)) + 1\n",
    "                      logger.info(f\"Found last page index {page_match.group(1)} in pagination, total pages: {total_pages}\")\n",
    "                      return min(total_pages, self.max_pages)\n",
    "              \n",
    "              # Look for numbered page links\n",
    "              page_links = pagination.find_all('a')\n",
    "              page_numbers = []\n",
    "              for link in page_links:\n",
    "                  href = link.get('href', '')\n",
    "                  page_match = re.search(r'page=(\\d+)', href)\n",
    "                  if page_match:\n",
    "                      page_numbers.append(int(page_match.group(1)))\n",
    "              \n",
    "              if page_numbers:\n",
    "                  max_page_index = max(page_numbers)\n",
    "                  total_pages = max_page_index + 1\n",
    "                  logger.info(f\"Found max page index {max_page_index} in links, total pages: {total_pages}\")\n",
    "                  return min(total_pages, self.max_pages)\n",
    "          \n",
    "          # Method 2: Check if there are results on the first page\n",
    "          rows = soup.find_all('div', class_='row')\n",
    "          document_count = 0\n",
    "          for row in rows:\n",
    "              main_col = row.find('div', class_='col-sm-8')\n",
    "              if main_col and main_col.find('div', class_='field-title'):\n",
    "                  document_count += 1\n",
    "          \n",
    "          if document_count == 0:\n",
    "              logger.warning(\"No documents found on first page\")\n",
    "              return 1\n",
    "          elif document_count < 1000:\n",
    "              logger.info(f\"Found {document_count} documents on first page (less than 1000), assuming single page\")\n",
    "              return 1\n",
    "          else:\n",
    "              logger.info(f\"Found {document_count} documents on first page, will test for more pages\")\n",
    "              # Test a few pages to find the actual limit\n",
    "              return self.test_page_availability()\n",
    "          \n",
    "      except Exception as e:\n",
    "          logger.error(f\"Error getting total pages: {e}\")\n",
    "          return 1\n",
    "  \n",
    "  def test_page_availability(self):\n",
    "      \"\"\"Test page availability to find the actual number of pages\"\"\"\n",
    "      logger.info(\"Testing page availability...\")\n",
    "      \n",
    "      # Binary search approach to find the last available page\n",
    "      left, right = 1, self.max_pages\n",
    "      last_valid_page = 1\n",
    "      \n",
    "      while left <= right:\n",
    "          mid = (left + right) // 2\n",
    "          test_url = f\"{self.base_url}&page={mid - 1}\"  # Convert to 0-indexed\n",
    "          \n",
    "          try:\n",
    "              response = self.session.get(test_url, timeout=10)\n",
    "              response.raise_for_status()\n",
    "              soup = BeautifulSoup(response.content, 'html.parser')\n",
    "              \n",
    "              # Check if this page has documents\n",
    "              rows = soup.find_all('div', class_='row')\n",
    "              has_documents = False\n",
    "              for row in rows:\n",
    "                  main_col = row.find('div', class_='col-sm-8')\n",
    "                  if main_col and main_col.find('div', class_='field-title'):\n",
    "                      has_documents = True\n",
    "                      break\n",
    "              \n",
    "              if has_documents:\n",
    "                  last_valid_page = mid\n",
    "                  left = mid + 1\n",
    "                  logger.info(f\"Page {mid} has documents\")\n",
    "              else:\n",
    "                  right = mid - 1\n",
    "                  logger.info(f\"Page {mid} has no documents\")\n",
    "              \n",
    "              time.sleep(0.5)  # Be respectful to the server\n",
    "              \n",
    "          except Exception as e:\n",
    "              logger.warning(f\"Error testing page {mid}: {e}\")\n",
    "              right = mid - 1\n",
    "      \n",
    "      logger.info(f\"Found {last_valid_page} total pages through testing\")\n",
    "      return last_valid_page\n",
    "  \n",
    "  def parse_date(self, date_string):\n",
    "      \"\"\"Parse different date formats\"\"\"\n",
    "      if not date_string:\n",
    "          return None\n",
    "      \n",
    "      try:\n",
    "          # Try to parse ISO format first\n",
    "          if 'T' in date_string:\n",
    "              return datetime.fromisoformat(date_string.replace('Z', '+00:00')).strftime('%B %d, %Y')\n",
    "          else:\n",
    "              # Try to parse as regular date\n",
    "              return datetime.strptime(date_string, '%Y-%m-%d').strftime('%B %d, %Y')\n",
    "      except:\n",
    "          return date_string.strip()\n",
    "  \n",
    "  def scrape_page(self, page_num):\n",
    "      \"\"\"Scrape a single page of documents (page_num is 1-indexed)\"\"\"\n",
    "      page_documents = []\n",
    "      \n",
    "      try:\n",
    "          # Convert to 0-indexed for URL\n",
    "          page_index = page_num - 1\n",
    "          \n",
    "          if page_index == 0:\n",
    "              url = self.base_url\n",
    "          else:\n",
    "              url = f\"{self.base_url}&page={page_index}\"\n",
    "          \n",
    "          logger.info(f\"Scraping page {page_num} (URL page={page_index}): {url}\")\n",
    "          \n",
    "          response = self.session.get(url, timeout=15)\n",
    "          response.raise_for_status()\n",
    "          soup = BeautifulSoup(response.content, 'html.parser')\n",
    "          \n",
    "          # Find all document rows\n",
    "          rows = soup.find_all('div', class_='row')\n",
    "          \n",
    "          documents_found = 0\n",
    "          for row in rows:\n",
    "              try:\n",
    "                  # Find the main content column (col-sm-8)\n",
    "                  main_col = row.find('div', class_='col-sm-8')\n",
    "                  related_col = row.find('div', class_='col-sm-4')\n",
    "                  \n",
    "                  if not main_col:\n",
    "                      continue\n",
    "                  \n",
    "                  # Check if this row contains a document (has field-title)\n",
    "                  title_element = main_col.find('div', class_='field-title')\n",
    "                  if not title_element:\n",
    "                      continue\n",
    "                  \n",
    "                  # Extract date\n",
    "                  date_element = main_col.find('span', {'property': 'dc:date'})\n",
    "                  if date_element:\n",
    "                      date_content = date_element.get('content', '')\n",
    "                      date_text = date_element.get_text(strip=True)\n",
    "                      formatted_date = self.parse_date(date_content) if date_content else date_text\n",
    "                  else:\n",
    "                      # Try to find date in h4 tag\n",
    "                      h4_element = main_col.find('h4')\n",
    "                      formatted_date = h4_element.get_text(strip=True) if h4_element else 'No Date'\n",
    "                  \n",
    "                  # Extract title and link\n",
    "                  link_element = title_element.find('a')\n",
    "                  if link_element:\n",
    "                      title = link_element.get_text(strip=True)\n",
    "                      document_link = urljoin('https://www.presidency.ucsb.edu', link_element.get('href', ''))\n",
    "                  else:\n",
    "                      title = title_element.get_text(strip=True)\n",
    "                      document_link = 'No Link'\n",
    "                  \n",
    "                  # Extract related information\n",
    "                  related_category = 'No Category'\n",
    "                  related_link = 'No Link'\n",
    "                  \n",
    "                  if related_col:\n",
    "                      related_link_element = related_col.find('a')\n",
    "                      if related_link_element:\n",
    "                          related_category = related_link_element.get_text(strip=True)\n",
    "                          related_link = urljoin('https://www.presidency.ucsb.edu', related_link_element.get('href', ''))\n",
    "                  \n",
    "                  # Create document record\n",
    "                  document = {\n",
    "                      'Date': formatted_date,\n",
    "                      'Title': title,\n",
    "                      'Document_Link': document_link,\n",
    "                      'Related_Category': related_category,\n",
    "                      'Related_Link': related_link,\n",
    "                      'Page': page_num,\n",
    "                      'URL_Page_Index': page_index\n",
    "                  }\n",
    "                  \n",
    "                  page_documents.append(document)\n",
    "                  documents_found += 1\n",
    "                  \n",
    "              except Exception as e:\n",
    "                  logger.warning(f\"Error parsing document on page {page_num}: {e}\")\n",
    "                  continue\n",
    "          \n",
    "          logger.info(f\"Page {page_num}: Found {documents_found} documents\")\n",
    "          \n",
    "          # Add small delay to be respectful to the server\n",
    "          time.sleep(0.3)\n",
    "          \n",
    "      except Exception as e:\n",
    "          logger.error(f\"Error scraping page {page_num}: {e}\")\n",
    "      \n",
    "      return page_documents\n",
    "  \n",
    "  def scrape_all_pages(self):\n",
    "      \"\"\"Scrape all pages using threading\"\"\"\n",
    "      logger.info(\"Determining total number of pages...\")\n",
    "      total_pages = self.get_total_pages()\n",
    "      logger.info(f\"Will scrape {total_pages} pages (each page has up to 1000 documents)\")\n",
    "      \n",
    "      # Limit max_workers to avoid overwhelming the server\n",
    "      actual_workers = min(self.max_workers, total_pages, 6)\n",
    "      logger.info(f\"Using {actual_workers} worker threads\")\n",
    "      \n",
    "      # Use ThreadPoolExecutor for concurrent scraping\n",
    "      with ThreadPoolExecutor(max_workers=actual_workers) as executor:\n",
    "          # Submit all page scraping tasks (1-indexed page numbers)\n",
    "          future_to_page = {\n",
    "              executor.submit(self.scrape_page, page_num): page_num \n",
    "              for page_num in range(1, total_pages + 1)\n",
    "          }\n",
    "          \n",
    "          # Collect results as they complete\n",
    "          completed_pages = 0\n",
    "          for future in as_completed(future_to_page):\n",
    "              page_num = future_to_page[future]\n",
    "              try:\n",
    "                  page_documents = future.result()\n",
    "                  with self.data_lock:\n",
    "                      self.all_documents.extend(page_documents)\n",
    "                  completed_pages += 1\n",
    "                  logger.info(f\"Progress: {completed_pages}/{total_pages} pages completed ({len(self.all_documents)} total documents)\")\n",
    "              except Exception as e:\n",
    "                  logger.error(f\"Error processing page {page_num}: {e}\")\n",
    "                  completed_pages += 1\n",
    "      \n",
    "      logger.info(f\"Scraping completed. Total documents found: {len(self.all_documents)}\")\n",
    "      return self.all_documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the scraper\n",
    "base_url = \"https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/campaign-documents?items_per_page=1000\"\n",
    "scraper = CampaignDocumentsScraper(base_url, max_workers=5, max_pages=25)\n",
    "\n",
    "# Start scraping\n",
    "start_time = time.time()\n",
    "logger.info(\"Starting campaign documents scraping...\")\n",
    "logger.info(f\"Base URL: {base_url}\")\n",
    "logger.info(\"Expected: Up to 25 pages with 1000 documents each\")\n",
    "\n",
    "documents = scraper.scrape_all_pages()\n",
    "\n",
    "end_time = time.time()\n",
    "scraping_time = end_time - start_time\n",
    "logger.info(f\"Scraping completed in {scraping_time:.2f} seconds\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "if not df.empty:\n",
    "    # Sort by page and then by date\n",
    "    df = df.sort_values(['Page', 'Date'], ascending=[True, False])\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nðŸ“Š **Campaign Documents Scraping Summary**\")\n",
    "    print(f\"Total documents scraped: {len(df):,}\")\n",
    "    print(f\"Pages scraped: {df['Page'].nunique()}\")\n",
    "    print(f\"Expected max documents (25 pages Ã— 1000): {25 * 1000:,}\")\n",
    "    print(f\"Actual coverage: {len(df) / (25 * 1000) * 100:.1f}% of maximum possible\")\n",
    "    print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "    print(f\"Scraping time: {scraping_time:.2f} seconds\")\n",
    "    print(f\"Average documents per page: {len(df) / df['Page'].nunique():.0f}\")\n",
    "    print(f\"Scraping speed: {len(df) / scraping_time:.1f} documents/second\")\n",
    "    \n",
    "    # Show page distribution\n",
    "    print(f\"\\nðŸ“„ **Documents per page:**\")\n",
    "    page_counts = df['Page'].value_counts().sort_index()\n",
    "    for page, count in page_counts.items():\n",
    "        print(f\"Page {page}: {count:,} documents\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nðŸ” **Sample Data (first 5 documents):**\")\n",
    "    sample_columns = ['Date', 'Title', 'Related_Category', 'Page']\n",
    "    print(df[sample_columns].head().to_string(index=False, max_colwidth=60))\n",
    "    \n",
    "    # Export to CSV\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f'./campaign_documents_{len(df)}docs_{timestamp}.csv'\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nðŸ’¾ **Data exported to:** {csv_filename}\")\n",
    "    \n",
    "    # Show data info\n",
    "    print(f\"\\nðŸ“ˆ **DataFrame Info:**\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Show unique related categories\n",
    "    print(f\"\\nðŸ·ï¸ **Top Related Categories:**\")\n",
    "    category_counts = df['Related_Category'].value_counts().head(10)\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  {category}: {count:,} documents\")\n",
    "    \n",
    "    # Show date distribution\n",
    "    print(f\"\\nðŸ“… **Document Distribution by Year:**\")\n",
    "    df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
    "    year_counts = df['Year'].value_counts().sort_index().tail(10)\n",
    "    for year, count in year_counts.items():\n",
    "        if pd.notna(year):\n",
    "            print(f\"  {int(year)}: {count:,} documents\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No documents found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./campaign_documents.csv')\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df = df[df.Date.between('2016-01-01', '2024-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fcf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create the sample CSV file to test with\n",
    "import pandas as pd\n",
    "\n",
    "sample_data = {\n",
    "    'Date': [\"September 29, 2024\", \"September 27, 2024\"],\n",
    "    'Title': [\n",
    "        \"Remarks by the Vice President at a Campaign Event in Las Vegas, Nevada\",\n",
    "        \"Remarks by the Vice President at a Campaign Event in Douglas, Arizona\"\n",
    "    ],\n",
    "    'Document_Link': [\n",
    "        \"https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-campaign-event-las-vegas-nevada-0\",\n",
    "        \"https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-campaign-event-douglas-arizona\"\n",
    "    ],\n",
    "    'Related_Category': [\"Kamala Harris\", \"Kamala Harris\"],\n",
    "    'Related_Link': [\n",
    "        \"https://www.presidency.ucsb.edu/people/other/kamala-harris\",\n",
    "        \"https://www.presidency.ucsb.edu/people/other/kamala-harris\"\n",
    "    ],\n",
    "    'Page': [1, 1],\n",
    "    'URL_Page_Index': [0, 0]\n",
    "}\n",
    "\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "df_sample.to_csv('./sample_documents.csv', index=False)\n",
    "\n",
    "print(\"Sample CSV created with your data structure:\")\n",
    "print(df_sample)\n",
    "print(f\"\\nColumns: {list(df_sample.columns)}\")\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OptimizedDocumentExtractor:\n",
    "    def __init__(self, max_workers=5, cache_file='extraction_cache.pkl', checkpoint_file='checkpoint.json'):\n",
    "        self.max_workers = max_workers\n",
    "        self.cache_file = cache_file\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.session = self._create_session()\n",
    "        self.cache = self._load_cache()\n",
    "        \n",
    "    def _create_session(self):\n",
    "        \"\"\"Create a session with retry strategy and connection pooling\"\"\"\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Retry strategy\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        \n",
    "        adapter = HTTPAdapter(\n",
    "            max_retries=retry_strategy,\n",
    "            pool_connections=20,\n",
    "            pool_maxsize=20\n",
    "        )\n",
    "        \n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        \n",
    "        session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        })\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load previously extracted data from cache\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'rb') as f:\n",
    "                    cache = pickle.load(f)\n",
    "                logger.info(f\"Loaded {len(cache)} cached entries\")\n",
    "                return cache\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to disk\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'wb') as f:\n",
    "                pickle.dump(self.cache, f)\n",
    "            logger.info(f\"Saved {len(self.cache)} entries to cache\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not save cache: {e}\")\n",
    "    \n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\"Load processing checkpoint\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load checkpoint: {e}\")\n",
    "        return {'processed_urls': [], 'last_index': 0}\n",
    "    \n",
    "    def _save_checkpoint(self, processed_urls, last_index):\n",
    "        \"\"\"Save processing checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'processed_urls': processed_urls,\n",
    "                'last_index': last_index,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not save checkpoint: {e}\")\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def _parse_date(self, date_string):\n",
    "        \"\"\"Cached date parsing function\"\"\"\n",
    "        if not date_string:\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            if 'T' in date_string and '+' in date_string:\n",
    "                parsed_date = datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "                return parsed_date.isoformat()\n",
    "            else:\n",
    "                date_formats = ['%B %d, %Y', '%m/%d/%Y', '%Y-%m-%d', '%d %B %Y']\n",
    "                for fmt in date_formats:\n",
    "                    try:\n",
    "                        parsed_date = datetime.strptime(date_string, fmt)\n",
    "                        return parsed_date.isoformat()\n",
    "                    except:\n",
    "                        continue\n",
    "                return date_string\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Date parsing error for '{date_string}': {e}\")\n",
    "            return date_string\n",
    "    \n",
    "    def extract_document_info(self, url):\n",
    "        \"\"\"Optimized document information extraction\"\"\"\n",
    "        # Check cache first\n",
    "        if url in self.cache:\n",
    "            logger.debug(f\"Cache hit for {url}\")\n",
    "            return self.cache[url]\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            result = {\n",
    "                'Document_Title': '',\n",
    "                'Document_Date': '',\n",
    "                'Document_Content': '',\n",
    "                'Speaker': '',\n",
    "                'Speaker_Title': '',\n",
    "                'Document_Type': '',\n",
    "                'Location': '',\n",
    "                'Video_Available': False,\n",
    "                'Word_Count': 0,\n",
    "                'Extraction_Status': 'Success'\n",
    "            }\n",
    "            \n",
    "            # Extract document title - prioritized selectors\n",
    "            title_element = (soup.select_one('.field-ds-doc-title h1') or \n",
    "                           soup.select_one('h1') or \n",
    "                           soup.select_one('.field-title h1'))\n",
    "            \n",
    "            if title_element:\n",
    "                result['Document_Title'] = title_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract document date - prioritized selectors\n",
    "            date_element = (soup.select_one('span[property=\"dc:date\"]') or \n",
    "                          soup.select_one('.field-docs-start-date-time span') or\n",
    "                          soup.select_one('.date-display-single'))\n",
    "            \n",
    "            if date_element:\n",
    "                date_content = date_element.get('content') or date_element.get_text(strip=True)\n",
    "                result['Document_Date'] = self._parse_date(date_content)\n",
    "            \n",
    "            # Extract speaker information\n",
    "            speaker_element = (soup.select_one('.field-title a') or \n",
    "                             soup.select_one('.diet-title a') or\n",
    "                             soup.select_one('h3 a'))\n",
    "            \n",
    "            if speaker_element:\n",
    "                result['Speaker'] = speaker_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract speaker title\n",
    "            title_element = (soup.select_one('.diet-by-line') or \n",
    "                           soup.select_one('.field-resuable-byline'))\n",
    "            \n",
    "            if title_element:\n",
    "                result['Speaker_Title'] = title_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract document content - optimized\n",
    "            content_element = soup.select_one('.field-docs-content')\n",
    "            if content_element:\n",
    "                # Remove unwanted elements\n",
    "                for unwanted in content_element(['script', 'style', 'nav', 'header', 'footer']):\n",
    "                    unwanted.decompose()\n",
    "                \n",
    "                # Extract text more efficiently\n",
    "                paragraphs = content_element.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "                    result['Document_Content'] = '\\\\n\\\\n'.join(content_parts)\n",
    "                    result['Word_Count'] = len(' '.join(content_parts).split())\n",
    "            \n",
    "            # Quick checks for other attributes\n",
    "            if soup.select_one('iframe[src*=\"youtube\"], iframe[src*=\"vimeo\"], .embedded-video'):\n",
    "                result['Video_Available'] = True\n",
    "            \n",
    "            # Extract location from title\n",
    "            title_lower = result['Document_Title'].lower()\n",
    "            location_patterns = [\n",
    "                r'in ([A-Z][a-z]+(?:\\\\s+[A-Z][a-z]+)*)',\n",
    "                r'at the ([A-Z][a-z]+(?:\\\\s+[A-Z][a-z]+)*)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in location_patterns:\n",
    "                match = re.search(pattern, result['Document_Title'])\n",
    "                if match:\n",
    "                    result['Location'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Determine document type\n",
    "            if 'debate' in title_lower:\n",
    "                result['Document_Type'] = 'Debate'\n",
    "            elif any(word in title_lower for word in ['remarks', 'speech']):\n",
    "                result['Document_Type'] = 'Speech/Remarks'\n",
    "            elif 'interview' in title_lower:\n",
    "                result['Document_Type'] = 'Interview'\n",
    "            elif 'statement' in title_lower:\n",
    "                result['Document_Type'] = 'Statement'\n",
    "            elif 'address' in title_lower:\n",
    "                result['Document_Type'] = 'Address'\n",
    "            else:\n",
    "                result['Document_Type'] = 'Document'\n",
    "            \n",
    "            # Cache the result\n",
    "            self.cache[url] = result\n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Network error for {url}: {e}\")\n",
    "            return self._create_error_result('Network Error')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing error for {url}: {e}\")\n",
    "            return self._create_error_result('Parsing Error')\n",
    "    \n",
    "    def _create_error_result(self, status):\n",
    "        \"\"\"Create error result structure\"\"\"\n",
    "        return {\n",
    "            'Document_Title': '',\n",
    "            'Document_Date': '',\n",
    "            'Document_Content': '',\n",
    "            'Speaker': '',\n",
    "            'Speaker_Title': '',\n",
    "            'Document_Type': '',\n",
    "            'Location': '',\n",
    "            'Video_Available': False,\n",
    "            'Word_Count': 0,\n",
    "            'Extraction_Status': status\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, urls_batch):\n",
    "        \"\"\"Process a batch of URLs\"\"\"\n",
    "        results = {}\n",
    "        for url in urls_batch:\n",
    "            results[url] = self.extract_document_info(url)\n",
    "            time.sleep(0.5)  # Reduced delay for batched processing\n",
    "        return results\n",
    "    \n",
    "    def process_documents_csv(self, csv_file_path, batch_size=50, save_interval=500):\n",
    "        \"\"\"\n",
    "        Process CSV file with optimizations for large datasets\n",
    "        \"\"\"\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        logger.info(f\"Processing {len(df)} rows...\")\n",
    "        logger.info(f\"Columns in CSV: {list(df.columns)}\")\n",
    "        \n",
    "        # Check if Document_Link column exists\n",
    "        if 'Document_Link' not in df.columns:\n",
    "            logger.error(\"Error: 'Document_Link' column not found in CSV\")\n",
    "            return None\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = self._load_checkpoint()\n",
    "        processed_urls = set(checkpoint.get('processed_urls', []))\n",
    "        last_index = checkpoint.get('last_index', 0)\n",
    "        \n",
    "        # Get unique URLs that haven't been processed\n",
    "        all_urls = df['Document_Link'].dropna().unique()\n",
    "        remaining_urls = [url for url in all_urls if url not in processed_urls]\n",
    "        \n",
    "        logger.info(f\"Total unique URLs: {len(all_urls)}\")\n",
    "        logger.info(f\"Already processed: {len(processed_urls)}\")\n",
    "        logger.info(f\"Remaining to process: {len(remaining_urls)}\")\n",
    "        \n",
    "        if not remaining_urls:\n",
    "            logger.info(\"All URLs already processed!\")\n",
    "        else:\n",
    "            # Process in batches with threading\n",
    "            url_batches = [remaining_urls[i:i + batch_size] for i in range(0, len(remaining_urls), batch_size)]\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                batch_futures = []\n",
    "                \n",
    "                for i, batch in enumerate(url_batches):\n",
    "                    future = executor.submit(self.process_batch, batch)\n",
    "                    batch_futures.append((i, future))\n",
    "                \n",
    "                # Process results as they complete\n",
    "                for i, future in tqdm(batch_futures, desc=\"Processing batches\"):\n",
    "                    try:\n",
    "                        batch_results = future.result(timeout=300)  # 5 minute timeout per batch\n",
    "                        \n",
    "                        # Update cache and processed URLs\n",
    "                        self.cache.update(batch_results)\n",
    "                        processed_urls.update(batch_results.keys())\n",
    "                        \n",
    "                        # Save progress periodically\n",
    "                        if (i + 1) % (save_interval // batch_size) == 0:\n",
    "                            self._save_cache()\n",
    "                            self._save_checkpoint(list(processed_urls), i * batch_size)\n",
    "                            logger.info(f\"Saved progress: {len(processed_urls)} URLs processed\")\n",
    "                        \n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        logger.error(f\"Batch {i} timed out\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing batch {i}: {e}\")\n",
    "        \n",
    "        # Final save\n",
    "        self._save_cache()\n",
    "        self._save_checkpoint(list(processed_urls), len(all_urls))\n",
    "        \n",
    "        # Add extracted information to the dataframe\n",
    "        new_columns = ['Document_Title', 'Document_Date', 'Document_Content', 'Speaker', \n",
    "                      'Speaker_Title', 'Document_Type', 'Location', 'Video_Available', \n",
    "                      'Word_Count', 'Extraction_Status']\n",
    "        \n",
    "        for col in new_columns:\n",
    "            df[col] = df['Document_Link'].map(lambda x: self.cache.get(x, {}).get(col, ''))\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        self._generate_summary(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _generate_summary(self, df):\n",
    "        \"\"\"Generate processing summary\"\"\"\n",
    "        total_rows = len(df)\n",
    "        successful_extractions = len(df[df['Extraction_Status'] == 'Success'])\n",
    "        unique_speakers = df['Speaker'].nunique()\n",
    "        total_words = df['Word_Count'].sum()\n",
    "        \n",
    "        logger.info(\"\\\\n\" + \"=\"*50)\n",
    "        logger.info(\"EXTRACTION SUMMARY\")\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(f\"Total rows processed: {total_rows}\")\n",
    "        logger.info(f\"Successful extractions: {successful_extractions} ({successful_extractions/total_rows*100:.1f}%)\")\n",
    "        logger.info(f\"Unique speakers found: {unique_speakers}\")\n",
    "        logger.info(f\"Total words extracted: {total_words:,}\")\n",
    "        logger.info(f\"Average words per document: {total_words/successful_extractions:.0f}\")\n",
    "        logger.info(f\"Documents with video: {df['Video_Available'].sum()}\")\n",
    "        \n",
    "        # Document type distribution\n",
    "        doc_types = df['Document_Type'].value_counts()\n",
    "        logger.info(\"\\\\nDocument Type Distribution:\")\n",
    "        for doc_type, count in doc_types.items():\n",
    "            logger.info(f\"  {doc_type}: {count}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run the extraction\"\"\"\n",
    "# Initialize extractor\n",
    "extractor = OptimizedDocumentExtractor(\n",
    "    max_workers=8,  # Adjust based on your system and respect for the server\n",
    "    cache_file='document_cache.pkl',\n",
    "    checkpoint_file='extraction_checkpoint.json'\n",
    ")\n",
    "\n",
    "# Process the CSV file\n",
    "csv_file = './campaign_documents.csv'  # Replace with your file name\n",
    "\n",
    "try:\n",
    "    processed_df = extractor.process_documents_csv(\n",
    "        csv_file, \n",
    "        batch_size=50,  # Process 50 URLs per batch\n",
    "        save_interval=500  # Save progress every 500 URLs\n",
    "    )\n",
    "    \n",
    "    if processed_df is not None:\n",
    "        # Save the final result\n",
    "        output_file = './campaign_documents.csv'\n",
    "        processed_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"\\\\nProcessing complete! Results saved to: {output_file}\")\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"\\\\nProcessing interrupted by user. Progress has been saved.\")\n",
    "    logger.info(\"You can resume processing by running the script again.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Fatal error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
